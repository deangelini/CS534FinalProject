# -*- coding: utf-8 -*-
"""tensorFruitCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k2YqobrELB-h8EqZEUGxvK2gKxA1U2gC

This code implements an AlexNet CNN. The CNN performs binary classification of fruit, indicating either fresh or rotten.
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

"""This section requires the user to input the directory containing the training and testing datasets. The training and testing sets must each have two folders: rotten and fresh, to hold images of the respective image classes."""

# folders would have pictures for their respective class
# train_dir = "/content/drive/MyDrive/AI_class/dataset/train/train_oranges"
train_dir = "/content/drive/MyDrive/TensorFlowCNN/dataset/train"

#Define the directory for testing images (format to train_dir): 
# test_dir = '/content/drive/MyDrive/AI_class/dataset/test/test_oranges'
test_dir = '/content/drive/MyDrive/TensorFlowCNN/dataset/test'

"""This section preprocesses the images and implements the CNN"""

from keras import callbacks
from keras import optimizers
import matplotlib.pyplot as plt
npix = 256 #define width x heigth of images for CNN i.e. size = npix x npix 

# Define the ImageDataGenerator and specify the augmentation parameters
train_datagen = ImageDataGenerator(
    rescale=1./255, # normalize pixel values to [0,1]
    rotation_range=20, # rotate the images randomly by up to 20 degrees
    width_shift_range=0.2, # shift the images horizontally by up to 20% of the width
    height_shift_range=0.2, # shift the images vertically by up to 20% of the height
    shear_range=0.2, # shear the images randomly by up to 20%
    zoom_range=0.2, # zoom the images randomly by up to 20%
    horizontal_flip=True, # flip the images horizontally randomly
    vertical_flip=True, # flip the images vertically randomly
    fill_mode='nearest', # fill in any empty pixels with the nearest neighbor
    validation_split = 0.2 #specify training data used for validation
)

# Generate the training data from the images in train directory
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(npix, npix), # image size
    class_mode='binary', # or categorical classification
    color_mode = "rgb",
    shuffle=True, # shuffle the data
    subset ='training'
)

# Generate the validation data from the images in train directory
val_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(npix, npix), 
    class_mode='binary',
    color_mode = "rgb",
    shuffle=True,
    subset='validation'  # specify to use the validation subset
)

# Define the AlexNet CNN model layers
model = tf.keras.Sequential([
    # First convolutional layer
    tf.keras.layers.Conv2D(96, (11,11), strides=(4,4), activation='relu', input_shape=(npix, npix, 3)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2)),
    
    # Second convolutional layer
    tf.keras.layers.Conv2D(256, (5,5), activation='relu', padding='same'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2)),
    
    # Third convolutional layer
    tf.keras.layers.Conv2D(384, (3,3), activation='relu', padding='same'),
    
    # Fourth convolutional layer
    tf.keras.layers.Conv2D(384, (3,3), activation='relu', padding='same'),
    
    # Fifth convolutional layer
    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2)),
    
    # Flatten the output from the convolutional layers
    tf.keras.layers.Flatten(),
    
    # First fully connected layer
    tf.keras.layers.Dense(4096, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    
    # Second fully connected layer
    tf.keras.layers.Dense(4096, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    
    # Output layer
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer = optimizers.Adam(learning_rate=0.000001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model using the training data
earlystopping = callbacks.EarlyStopping(monitor="accuracy",
                                        mode="max", min_delta=1,
                                        restore_best_weights=True)

history = model.fit(train_generator, validation_data=val_generator, steps_per_epoch = train_generator.samples/train_generator.batch_size, 
                    validation_steps=val_generator.samples/val_generator.batch_size, epochs=32)

# Now test the CNN

# Define the ImageDataGenerator for the test images and normalize the pixel values
test_datagen = ImageDataGenerator(rescale=1./255)

# Generate the test data from the images in the directory
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(npix, npix),
    #batch_size= 100, # since you have 6 test images, you can set the batch size to the number of images (100 apple images)
    class_mode='binary',
    color_mode = "rgb",
    shuffle=False, # don't shuffle the data
)
# Evaluate the trained CNN:
test_loss, test_acc = model.evaluate(test_generator, steps = test_generator.samples/test_generator.batch_size)
print('Test accuracy:', test_acc)

#Tracking Loss with Number of Epochs
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training','Validation'])
plt.title("Training And Validation Loss")
plt.xlabel("Epochs")

"""GridSearchCV"""

from sklearn.model_selection import GridSearchCV
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from keras import callbacks
from keras import optimizers
import matplotlib.pyplot as plt
npix = 256 #define width x heigth of images for CNN i.e. size = npix x npix 

# Define the ImageDataGenerator and specify the augmentation parameters
train_datagen = ImageDataGenerator(
    rescale=1./255, # normalize pixel values to [0,1]
    rotation_range=20, # rotate the images randomly by up to 20 degrees
    width_shift_range=0.2, # shift the images horizontally by up to 20% of the width
    height_shift_range=0.2, # shift the images vertically by up to 20% of the height
    shear_range=0.2, # shear the images randomly by up to 20%
    zoom_range=0.2, # zoom the images randomly by up to 20%
    horizontal_flip=True, # flip the images horizontally randomly
    vertical_flip=True, # flip the images vertically randomly
    fill_mode='nearest', # fill in any empty pixels with the nearest neighbor
    validation_split = 0.2 #specify training data used for validation
)

# Generate the training data from the images in train directory
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(npix, npix), # image size
    class_mode='binary', # or categorical classification
    color_mode = "rgb",
    shuffle=True, # shuffle the data
    subset ='training'
)

# Generate the validation data from the images in train directory
val_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(npix, npix), 
    class_mode='binary',
    color_mode = "rgb",
    shuffle=True,
    subset='validation'  # specify to use the validation subset
)

# Define the function to create the CNN model
def create_model(learning_rate=0.000001, dropout_rate=0.5, optimizer='adam'):
  model = tf.keras.Sequential([
      # First convolutional layer
      tf.keras.layers.Conv2D(96, (11,11), strides=(4,4), activation='relu', input_shape=(npix, npix, 3)),
      tf.keras.layers.BatchNormalization(),
      tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2)),

      # Second convolutional layer
      tf.keras.layers.Conv2D(256, (5,5), activation='relu', padding='same'),
      tf.keras.layers.BatchNormalization(),
      tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2)),

      # Third convolutional layer
      tf.keras.layers.Conv2D(384, (3,3), activation='relu', padding='same'),

      # Fourth convolutional layer
      tf.keras.layers.Conv2D(384, (3,3), activation='relu', padding='same'),

      # Fifth convolutional layer
      tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),
      tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2)),

      # Flatten the output from the convolutional layers
      tf.keras.layers.Flatten(),

      # First fully connected layer
      tf.keras.layers.Dense(4096, activation='relu'),
      tf.keras.layers.Dropout(dropout_rate),

      # Second fully connected layer
      tf.keras.layers.Dense(4096, activation='relu'),
      tf.keras.layers.Dropout(dropout_rate),

      # Output layer
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  
  # Compile the model
  model.compile(optimizer=optimizer, 
                loss='binary_crossentropy', 
                metrics=['accuracy'])

  return model

# Create a KerasClassifier object to wrap the model and use with GridSearchCV
model = KerasClassifier(build_fn=create_model)

# Define early stopping
earlystopping = callbacks.EarlyStopping(monitor="accuracy",mode="max", min_delta=1,restore_best_weights=True)

# Define the hyperparameters to tune
param_grid = {
    'learning_rate': [0.0001, 0.00001, 0.000001],
    'dropout_rate': [0.3, 0.5, 0.7],
    'optimizer': ['adam', 'rmsprop', 'adagrad']
}

# Create the GridSearchCV object
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1)

# Fit the GridSearchCV object to the training data
grid_result = grid.fit(train_generator, validation_data=val_generator, steps_per_epoch=train_generator.samples/train_generator.batch_size,
                        validation_steps=val_generator.samples/val_generator.batch_size, epochs=32, callbacks=[earlystopping])

# Print the best hyperparameters
print("Best Parameters: ", grid_result.best_params_)

# Evaluate the best model on the test data
best_model = grid_result.best_estimator_.model
test_loss, test_acc = best_model.evaluate(test_generator, steps=test_generator.samples/test_generator.batch_size)
print('Test accuracy:', test_acc)

#Tracking Loss with Number of Epochs
plt.plot(best_model.history.history['loss'])
plt.plot(best_model.history.history['val_loss'])
plt.legend(['Training','Validation'])
plt.title("Training And Validation Loss")
plt.xlabel("Epochs")